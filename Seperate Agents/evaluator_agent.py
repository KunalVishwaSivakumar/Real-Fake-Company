from crewai import Agent, Task, Crew
from crewai.tools import BaseTool
from dotenv import load_dotenv
import json
import os

load_dotenv()

# --- Load planner output ---
planner_path = "planner_results.json"

if not os.path.exists(planner_path):
    raise FileNotFoundError("planner_results.json not found. Run planner_agent.py first.")

with open(planner_path, "r", encoding="utf-8") as f:
    try:
        planner_data = json.load(f)
    except json.JSONDecodeError:
        raise ValueError("planner_results.json is not valid JSON.")

# --- Evaluation Logic ---
class EvaluationLogic:
    def __init__(self, plan):
        self.plan = plan

    def evaluate(self):
        actions = self.plan.get("actions", [])
        remarks = []
        score = 10.0  # Start with perfect score

        if not actions:
            return {
                "score": 0,
                "sop_compliance": "No actions present to evaluate.",
                "remarks": "Planner did not consolidate inputs or mitigation steps are missing."
            }

        for entry in actions:
            agent = entry.get("agent", "")
            action = entry.get("action", "").lower()

            if agent == "SchedulerAgent":
                if not any(word in action for word in ["delay", "reschedule", "coordinate"]):
                    remarks.append("SchedulerAgent action lacks clear delay or mitigation handling.")
                    score -= 1.5
            elif agent == "SafetyAgent":
                if not any(word in action for word in ["ppe", "safety", "supervisor", "training"]):
                    remarks.append("SafetyAgent action does not fully reflect safety SOP.")
                    score -= 1.5
            elif agent == "QAQCAgent":
                if not any(word in action for word in ["reapply", "inspection", "compliance"]):
                    remarks.append("QAQCAgent action lacks clear rework or inspection response.")
                    score -= 1.5

        if not remarks:
            remarks.append("All agent actions are SOP-aligned and clearly stated.")

        return {
            "score": round(max(score, 0), 2),
            "sop_compliance": "Yes" if score >= 9 else "Partial",
            "remarks": " ".join(remarks)
        }

# --- CrewAI Tool ---
class EvaluationTool(BaseTool):
    name: str = "EvaluatePlan"
    description: str = "Evaluates the final mitigation plan for completeness and SOP alignment."

    def _run(self, **kwargs) -> str:
        logic = EvaluationLogic(planner_data)
        return json.dumps(logic.evaluate(), indent=2)

# --- Agent ---
evaluator_agent = Agent(
    role="Project Evaluator",
    goal="Evaluate the final mitigation plan for quality and SOP alignment.",
    backstory="You're the final checkpoint. You analyze plan clarity, SOP compliance, and provide an overall quality score.",
    tools=[EvaluationTool()],
    verbose=True
)

# --- Task ---
evaluator_task = Task(
    description="Use the provided tool to evaluate the project mitigation plan generated by the PlannerAgent. Score plan quality and check for SOP compliance.",
    expected_output="Score (0-10), SOP compliance status, and feedback remarks.",
    agent=evaluator_agent,
    tool_choice="required"
)

# --- Run the Crew ---
crew = Crew(
    agents=[evaluator_agent],
    tasks=[evaluator_task],
    verbose=True
)

results = crew.kickoff()

# --- Save Output ---
try:
    parsed = json.loads(str(results))
except json.JSONDecodeError:
    parsed = {"error": "Could not parse evaluator output", "raw": str(results)}

with open("evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(parsed, f, indent=2)

# --- Print Result ---
print("\n--- Evaluation Output ---\n")
print(json.dumps(parsed, indent=2, ensure_ascii=False))
